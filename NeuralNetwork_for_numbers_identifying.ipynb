{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Libraries**"
      ],
      "metadata": {
        "id": "rbLcLVg6voOR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rs9jocdov7tk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load MNIST**"
      ],
      "metadata": {
        "id": "TQJEulPMvo94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = np.array( x_train,dtype = float )\n",
        "x_test = np.array( x_test,dtype = float )\n",
        "\n",
        "x_train = x_train.reshape(-1,784)\n",
        "x_test = x_test.reshape(-1,784) \n"
      ],
      "metadata": {
        "id": "hMs-tLS5wk1x"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standardization**"
      ],
      "metadata": {
        "id": "8XVWaCNivwZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train=y_train.reshape(60000,1) \n",
        "y_test=y_test.reshape(10000,1)\n",
        "\n",
        "x_train= (x_train - np.mean(x_train))/ np.std(x_train)\n",
        "x_test= (x_test - np.mean(x_test))/ np.std(x_test)\n"
      ],
      "metadata": {
        "id": "N-uGozMGwuCs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid**"
      ],
      "metadata": {
        "id": "X4oiG0XNv0ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sigmoid(z,der= False):#i need derivative in chain rule...\n",
        "  if der:\n",
        "     return (np.exp(-z))/((np.exp(-z)+1)**2)\n",
        "\n",
        "  return (1/(1+np.exp(-z)))\n"
      ],
      "metadata": {
        "id": "Gt787l6Hwur_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters initialization function**"
      ],
      "metadata": {
        "id": "ViDoscQtv_Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(size_of_layers):\n",
        "    parameters = {}\n",
        "    num_of_layers = len(size_of_layers)\n",
        "     \n",
        "    for i in range(1, num_of_layers):\n",
        "        # store w,b for each layer\n",
        "        # dict includes weights matrices with the number of layers\n",
        "        # dict includes biases with numb er of layers   \n",
        "        parameters['W' + str(i)] = np.random.randn(size_of_layers[i], size_of_layers[i-1])   * np.sqrt(1. / size_of_layers[i])# when neurons increased i want lower weights\n",
        "        parameters['B' + str(i)] = np.random.randn(size_of_layers[i], 1)     * np.sqrt(1. / size_of_layers[i])\n",
        "    \n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "HShkrUPm132z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward Propagation**"
      ],
      "metadata": {
        "id": "5zkOzILvwHeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def forwardprop(xtrain,parameters,size_of_layers):\n",
        "  \n",
        "  parameters['A0']= xtrain # features \n",
        "  parameters['A0']=parameters['A0'].reshape(784,1) # to avoid (784,)\n",
        "  num_of_layers = len(size_of_layers)\n",
        "\n",
        "  for i in range(1,num_of_layers):\n",
        "    parameters['Z'+str(i)] = np.dot(parameters['W'+str(i)],parameters['A'+str(i-1)])+parameters['B'+str(i)]\n",
        "    parameters['A'+str(i)] = sigmoid(parameters['Z'+str(i)])\n",
        "    \n",
        "  return parameters \n",
        "  "
      ],
      "metadata": {
        "id": "S5taWLVA3N7W"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backward Propagation**"
      ],
      "metadata": {
        "id": "hTqOyDjLwSGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#outer 18 23\n",
        "def backwardprop(ytrain,output,parameters,size_of_layers):\n",
        "  changes = {} # dw + db  \n",
        "  num_of_layers = len(size_of_layers)\n",
        "\n",
        "  #(a,b)*(b,d) = (a,d) dot product\n",
        "  #(a,b)*(c,d) = (a,d) outer\n",
        "    \n",
        "  # dl/dw = dl/da * da/z * a_prev  dz/dw      \n",
        "  # calculate output layer error first or dz\n",
        "  # dervative of mean square error has a summation but as i am calc the mse for each neuron individually no need for summation\n",
        "  # dl/da = 2*(a-y)/len(a) \n",
        "  # dz= 2*(a-y)/len(a)*sigmoid(z) # where sigmoid(z) without derivative is the A of the layer but i need derivative of sigmoid(z) \n",
        "  dz = 2*(output-ytrain)/output.shape[0]*sigmoid(parameters['Z'+str(num_of_layers-1)],der=True)\n",
        "\n",
        "  # dl/dw = dz * a_ofprev, donot forget eta later\n",
        "  #outer product is for shapes (10,1) (20,1)  -->> (10,1)   1 cuz of one sample\n",
        "  changes['W'+str(num_of_layers-1)]=np.outer(dz, parameters['A'+str(num_of_layers-2)])\n",
        "  changes['B'+str(num_of_layers-1)]=np.sum(dz, axis=1, keepdims=True)\n",
        "\n",
        "  for i in range(num_of_layers-2, 0, -1):\n",
        "     dz=np.dot(parameters['W'+str(i+1)].T,dz)*sigmoid(parameters['Z'+str(i)],der=True)\n",
        "     changes['W'+str(i)]=np.outer(dz,parameters['A'+str(i-1)])\n",
        "     changes['B'+str(i)]=np.sum(dz, axis=1, keepdims=True) \n",
        "  return changes\n"
      ],
      "metadata": {
        "id": "1e6A0YXJ77_Z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Update parameters function**"
      ],
      "metadata": {
        "id": "kFWyO3OPwaqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def update_parameters(changes,parameters,eta):\n",
        "    # I used stochastic gradient descent\n",
        "    for key,value in changes.items():\n",
        "      #because i stored all A and Z in this dictionary not only w&b\n",
        "      #so i need to update only w,b       \n",
        "      if(key[0]=='W' or key[0]=='B'): \n",
        "         parameters[key] -= eta * value\n",
        "      \n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "boRUoQByIFXh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction function**"
      ],
      "metadata": {
        "id": "16Aukkl5v5-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict(parameters,size_of_layers):\n",
        "    num_of_layers=len(size_of_layers)\n",
        "\n",
        "    predictions=[]\n",
        "    for j in range(len(y_test)):\n",
        "          input = np.asfarray(x_test[j])\n",
        "          target = np.zeros(10) + 0.01\n",
        "          target[y_test[j]] = 0.99\n",
        "          parameters = forwardprop(input,parameters,size_of_layers)\n",
        "          output = parameters['A'+str(num_of_layers-1)]\n",
        "          \n",
        "          ypred= np.argmax(output)\n",
        "          predictions.append(ypred==np.argmax(target))\n",
        "\n",
        "    return predictions \n"
      ],
      "metadata": {
        "id": "p4sxbyf5w20R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network function**"
      ],
      "metadata": {
        "id": "mqjZZ03eyFJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def NN (x, y, num_of_layers, size_of_layers): #size_of_layers= [784,20,10]   \n",
        "    \n",
        "    # add input layers and there sizes \n",
        "    num_of_layers+=1\n",
        "    size_of_layers.insert(0,784)\n",
        "    \n",
        "    epochs = 15\n",
        "    eta = 0.1\n",
        "    \n",
        "    # dicitionary of paramerters \n",
        "    parameters = initialize_parameters(size_of_layers)\n",
        "    \n",
        "    for i in range(epochs):\n",
        "        #pass sample by sample to neural network\n",
        "        for j in range(len(x)):\n",
        "          #take sample as input\n",
        "          input = np.asfarray(x[j])\n",
        "          input = input.reshape(784,1)\n",
        "          #take y as target\n",
        "          # oneHot encoding  \n",
        "          target = np.zeros(10) + 0.01\n",
        "          target[y[j]] = 0.99\n",
        "          target=target.reshape(10,1)\n",
        "          \n",
        "          # forward path \n",
        "          parameters = forwardprop(input,parameters,size_of_layers)\n",
        "          output = parameters['A'+str(num_of_layers-1)]\n",
        "  \n",
        "          # backward path\n",
        "          changes = backwardprop(target,output,parameters,size_of_layers)\n",
        "\n",
        "          # update parameters with stochastic gradient descent\n",
        "          parameters = update_parameters(changes,parameters,eta)\n",
        "    predictions = predict(parameters,size_of_layers)      \n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "MK3BSpzaw4qo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy Function**"
      ],
      "metadata": {
        "id": "sxRMuIBxv30_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def accuracy(predictions):\n",
        "    t=0\n",
        "    for x in predictions:\n",
        "       if x :\n",
        "         t+=1\n",
        "#np.mean(predictions) --> this gives an error of un being callable, so i stored it in variable first\n",
        "    return t/len(predictions) \n"
      ],
      "metadata": {
        "id": "PfDjoLmPw0rz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run 3 Neural Networks dynamically**"
      ],
      "metadata": {
        "id": "azjrQW_ivKAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_layers = [2,3,3]\n",
        "size_of_layers = [[30,10],[20,30,10],[80,50,10]]\n",
        "\n",
        "predictions1 = NN(x_train, y_train, num_of_layers[0], size_of_layers[0])\n",
        "predictions2 = NN(x_train, y_train, num_of_layers[1], size_of_layers[1])\n",
        "predictions3 = NN(x_train, y_train, num_of_layers[2], size_of_layers[2])\n"
      ],
      "metadata": {
        "id": "oGlUGUxESchR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Neural Networks**"
      ],
      "metadata": {
        "id": "bAh3UntavNqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy1=accuracy(predictions1)\n",
        "print(\"Accuracy of first Neural Network (1 hidden layer with neurons=[30] ):\", str(np.round(accuracy1*100,2))+\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDbViBCDgTpk",
        "outputId": "1d928628-3432-4e78-d469-bb124638efd4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of first Neural Network (1 hidden layer with neurons=[30] ): 94.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy2=accuracy(predictions2)\n",
        "print(\"Accuracy of second Neural Network (2 hidden layers with neurons=[20,30]):\",str(np.round(accuracy2*100,2))+\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvqW7XOP-72n",
        "outputId": "1b0c8050-7fe6-4939-f23c-89e74a6f9207"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of second Neural Network (2 hidden layers with neurons=[20,30]): 93.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy3=accuracy(predictions3)\n",
        "print(\"Accuracy of third Neural Network (2 hidden layers with neurons=[80,50]):\",str( np.round(accuracy3*100,2))+\"%\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFTf2t4j_LqT",
        "outputId": "9c8e8c46-d538-4f9f-bb12-2d9a8f6bdf20"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of third Neural Network (2 hidden layers with neurons=[80,50]): 97.1%\n"
          ]
        }
      ]
    }
  ]
}